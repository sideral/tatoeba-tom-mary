{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import time\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regexes to find Tom and Mary in several languages\n",
    "tom_mary = {\n",
    "    'Tom':   {\n",
    "        'eng': 'Tom',\n",
    "        'spa': 'Tom(ás)?',\n",
    "        'rus': 'Том(а|ом|у)?',\n",
    "        'ita': 'Tom(maso)?',\n",
    "        'fra': '(Tom|Thomas)',\n",
    "        'hat': 'Tom',\n",
    "        'por': 'Tom',\n",
    "        'deu': 'Tom(s)?',\n",
    "        'nld': '(Tom(s)?|Thomas)',\n",
    "        'dan': 'Tom(s)?',\n",
    "        'nob': 'Tom(s|i)?',\n",
    "        'swe': 'Tom(s|i)?',\n",
    "        'fin': \"Tom(m|')?(i(n|lla|a|lle|sta|lta|in|sta|ssa|kin|i|l|kaan|st|tta)?|kaan)?\"\n",
    "    },\n",
    "    'Mary': {\n",
    "        'eng': 'Mar(y|ia|ie)',\n",
    "        'spa': 'Mar(y|ía|ia)',\n",
    "        'fra': 'Mar(y|ie|ia)',\n",
    "        'rus': '(Мэри|Мари(я|и|у|ей)|Маш(у|а|ей|и))',\n",
    "        'ita': 'Mar(ie|ia|y|i)',\n",
    "        'hat': 'Mary',\n",
    "        'por': 'Mar(y|ia)',\n",
    "        'deu': 'Mar(y|ia|i)(s)?',\n",
    "        'nld': 'Mar(y|ia|ie|yam)',\n",
    "        'dan': 'Mar(y|ia|ie|i)(s)?',\n",
    "        'nob': 'Mar(y|ia|ie|i)(s)?',\n",
    "        'swe': 'Mar(y|ia|ie|i)(s)?',\n",
    "        'fin': 'Mar(i|y)(n|a|lle|sta|a|lla|lta|ä|ja|in|stä|yn|llä|aa|en)?'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Common names for substitution\n",
    "names = {\n",
    "    'eng': {\n",
    "        'male':  [\n",
    "            'Omar', 'Peter', 'Santiago', 'Daniel', 'William', 'Luis', 'James', 'John', 'Robert', 'Gabriel',\n",
    "            'Oliver', 'Jonas', 'Charlie', 'Jack', 'Leonardo', 'David', 'Alexander', 'Sergei', 'Abraham', 'Tatsuki'\n",
    "        ],\n",
    "        'female': [\n",
    "            'Anna', 'Emily', 'Natalia', 'Salma', 'Valentina', 'Olivia', 'Amelia', 'Viktoria', 'Anastasia',\n",
    "            'Maryam', 'Sakura', 'Charlotte', 'Sarah', 'Ashley', 'Samantha', 'Laura', 'Latifa', 'Carlota', 'Eva',\n",
    "            'Olga'\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    iso_map = {\n",
    "        \"eng\": \"en\",\n",
    "        \"rus\": \"ru\",\n",
    "        \"ita\": \"it\",\n",
    "        \"deu\": \"de\",\n",
    "        \"fra\": \"fr\",\n",
    "        \"por\": \"pt\",\n",
    "        \"spa\": \"es\",\n",
    "        \"nld\": \"nl\",\n",
    "        \"fin\": \"fi\",\n",
    "        \"dan\": \"da\",\n",
    "        \"swe\": \"sv\",\n",
    "        \"nob\": \"no\",\n",
    "        \"hat\": \"ht\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def __init__(self, *, data_path: str, name_patterns : dict, new_names: dict, seed: int = 42):\n",
    "        self.name_patterns = name_patterns\n",
    "        self.new_names = new_names\n",
    "        self.data_path = data_path\n",
    "        self.languages = list(self.iso_map.keys())\n",
    "        self.sentences = self._load_sentences()\n",
    "        self.translations = self._load_translations()\n",
    "        random.seed(seed)\n",
    "        \n",
    "        \n",
    "    def _load_sentences(self):\n",
    "        path = os.path.join(self.data_path, 'sentences.csv')\n",
    "        sentences = pd.read_csv(path, sep='\\t', names=['tatoeba_id', 'language', 'sentence'])\n",
    "        sentences = sentences[sentences['language'].isin(self.languages)]\n",
    "        return sentences.set_index('tatoeba_id', drop=False).sort_index()\n",
    "    \n",
    "    \n",
    "    def _load_translations(self):\n",
    "        path = os.path.join(self.data_path, 'links.csv')\n",
    "        translations = pd.read_csv(path, sep='\\t', names = ['source', 'target'])\n",
    "        translations = translations.set_index('target', drop=False).join(self.sentences).dropna()[[\n",
    "            'source', 'target', 'language'\n",
    "        ]].rename(columns = {'language': 'target_language'})\n",
    "\n",
    "        translations = translations.set_index('source', drop=False).join(self.sentences).dropna()[[\n",
    "            'source', 'target', 'language', 'target_language'\n",
    "        ]].rename(columns = {'language': 'source_language'})\n",
    "\n",
    "        return translations\n",
    "    \n",
    "    \n",
    "    def random_name(self, gender: str, lang: str):\n",
    "        return random.sample(self.new_names[lang][gender], 1)[0]\n",
    "    \n",
    "    \n",
    "    def name_sentences(self, name: str, lang: str):\n",
    "        pattern = self.name_patterns[name][lang]\n",
    "        lang_sentences = self.sentences.loc[self.sentences['language'] == lang]\n",
    "        name_regex = r'\\b' + pattern + r'\\b'\n",
    "        return lang_sentences[lang_sentences['sentence'].str.contains(name_regex, regex=True)]\n",
    "    \n",
    "    \n",
    "    def name_pattern(self, name: str, lang: str):\n",
    "        return self.name_patterns[name][lang]\n",
    "    \n",
    "    \n",
    "    def iso(self, lang: str):\n",
    "        return self.iso_map[lang]\n",
    "\n",
    "    \n",
    "    def find_translations(self, source_ids, tgt_lang):\n",
    "        ids = self.translations[\n",
    "            (self.translations['source'].isin(source_ids)) & (self.translations['target_language'] == 'eng')\n",
    "        ]\n",
    "        groups = ids.groupby(['source'])['target'].apply(list)\n",
    "        keys = []\n",
    "        mapping = {}\n",
    "        for source_id in source_ids:\n",
    "            try: \n",
    "                key = groups.loc[[source_id]].to_list()[0][0]\n",
    "                keys.append(key)\n",
    "                mapping[source_id] = key\n",
    "            except KeyError:\n",
    "                mapping[source_id] = None\n",
    "\n",
    "        targets = self.sentences.loc[keys]['sentence'].to_dict()\n",
    "        return {k: targets[v] if v is not None else None for (k,v) in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGenerator: \n",
    "    def __init__(self, *, data: DataSource, name: str, gender: str, init_lang: str, max_depth: int = 15):\n",
    "        self.data = data\n",
    "        self.replacement_map = {}\n",
    "        self.max_depth = max_depth\n",
    "        self.name = name\n",
    "        self.gender = gender\n",
    "        self.init_lang = init_lang\n",
    "        self.takeback_ids = []\n",
    "        \n",
    "        \n",
    "    def generate(self):\n",
    "        self._fill_replacement_map()\n",
    "        return len(self.replacement_map)\n",
    "    \n",
    "    \n",
    "    def save(self, data_path):\n",
    "        path = os.path.join(data_path, self.name.lower() + '_replacements.pkl')\n",
    "        with open(path, 'wb') as file:\n",
    "            pickle.dump(self.replacement_map, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    def load(self, data_path):\n",
    "        path = os.path.join(data_path, self.name.lower() + '_replacements.pkl')\n",
    "        with open(path, 'rb') as file:\n",
    "            self.replacement_map = pickle.load(file)\n",
    "    \n",
    "    \n",
    "    def get_name_replacement(self, tatoeba_id):\n",
    "        if tatoeba_id not in self.replacement_map:\n",
    "            self._fill_for_ids([tatoeba_id])\n",
    "        return self.replacement_map[tatoeba_id]\n",
    "    \n",
    "    \n",
    "    def takeback(self, tatoeba_id, depth=0):\n",
    "        if depth > self.max_depth:\n",
    "            return\n",
    "        if tatoeba_id not in self.takeback_ids:\n",
    "            self.takeback_ids.append(tatoeba_id)\n",
    "            translations = self.data.translations[self.data.translations['source'] == tatoeba_id]\n",
    "            self._takeback_translations(translations, depth)\n",
    "    \n",
    "    \n",
    "    def _takeback_translations(self, translations, depth):\n",
    "        for tatoeba_id in translations['target']:\n",
    "            self.takeback(tatoeba_id, depth+1)\n",
    "        \n",
    "\n",
    "    def _fill_replacement_map(self):\n",
    "        sentences = self.data.name_sentences(self.name, self.init_lang)\n",
    "        self._fill_for_ids(sentences['tatoeba_id'])\n",
    "    \n",
    "    \n",
    "    def _fill_for_ids(self, sentence_ids, depth=0):\n",
    "        if depth > self.max_depth:\n",
    "            return\n",
    "        for tatoeba_id in sentence_ids:\n",
    "            if tatoeba_id not in self.replacement_map:\n",
    "                translations = self.data.translations[self.data.translations['source'] == tatoeba_id]\n",
    "                self.replacement_map[tatoeba_id] = self._get_name(translations)\n",
    "                print(len(self.replacement_map), end='\\r')\n",
    "                self._fill_translations(translations, depth)\n",
    "            \n",
    "    \n",
    "    def _fill_translations(self, translations, depth):\n",
    "        for language in self.data.languages:\n",
    "            targets = translations[translations['target_language'] == language]['target']\n",
    "            self._fill_for_ids(targets, depth + 1)\n",
    "    \n",
    "    \n",
    "    def _get_name(self, translations):\n",
    "        # First try to find if some translation already has a name\n",
    "        for tatoeba_id in translations['target']:\n",
    "            if tatoeba_id in self.replacement_map:\n",
    "                return self.replacement_map[tatoeba_id]\n",
    "        # If not, generate a new name at random\n",
    "        return self.data.random_name(self.gender, self.init_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator: \n",
    "    models = {}\n",
    "    cache = {}\n",
    "\n",
    "    def translate(self, text: str, src_lang: str, tgt_lang: str):\n",
    "        cached = self.cache.pop(text, None)\n",
    "        if cached:\n",
    "            return cached\n",
    "        translations = self.translate_batch([text], src_lang, tgt_lang)\n",
    "        return translations[0]\n",
    "\n",
    "\n",
    "    def translate_batch(self, texts: list, src_lang: str, tgt_lang: str):\n",
    "        model, tokenizer = self._load_model(src_lang, tgt_lang)\n",
    "\n",
    "        # The target language is specified as a special token within the source string\n",
    "        batch = [f'>>{tgt_lang}<< {text}' for text in texts]\n",
    "\n",
    "        translated = model.generate(**tokenizer.prepare_translation_batch(batch))\n",
    "        translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "        # Resulting translations sometimes contain weird characters at the start, remove them\n",
    "        translations = [re.sub(r'^(▸|▪|::|\\-|□|–|\"|\\*|♪)\\s*', '', t) for t in translations]\n",
    "\n",
    "        translations = [\n",
    "            re.sub(r'\\.$', '', translations[i]) if texts[i][-1] != '.' else translations[i] \n",
    "            for i in range(len(translations))\n",
    "        ]\n",
    " \n",
    "        return translations\n",
    " \n",
    "\n",
    "    def cache_translations(self, texts: list, src_lang: str, tgt_lang: str):\n",
    "        translations = self.translate_batch(texts, src_lang, tgt_lang)\n",
    "            \n",
    "        for idx, translation in enumerate(translations):\n",
    "            self.cache[texts[idx]] = translation\n",
    "    \n",
    "    \n",
    "    def _get_model_name(self, src_lang, tgt_lang):\n",
    "        # There are only two models from romance languages to English and back\n",
    "        romance_langs = [\"fr\", \"es\", \"it\", \"pt\", \"ro\", \"ca\", \"gl\", \"la\", \"wa\", \"fur\", \"oc\", \"sc\", \"an\", \"frp\", \"lad\", \"vec\", \"co\", \"lld\", \"lij\", \"lmo\", \"nap\", \"rm\", \"scn\", \"mwl\"]\n",
    "        src_lang = 'ROMANCE' if src_lang in romance_langs and tgt_lang == 'en' else src_lang\n",
    "        tgt_lang = 'ROMANCE' if tgt_lang in romance_langs and src_lang == 'en' else tgt_lang\n",
    "        tgt_lang = 'NORWAY' if tgt_lang == 'no' else tgt_lang\n",
    "        return 'Helsinki-NLP/opus-mt-{0}-{1}'.format(src_lang, tgt_lang)\n",
    "\n",
    "\n",
    "    def _load_model(self, src_lang, tgt_lang):\n",
    "        model_name = self._get_model_name(src_lang, tgt_lang)\n",
    "        if model_name not in self.models:\n",
    "            print('Loading ' + model_name)\n",
    "            model = MarianMTModel.from_pretrained(model_name)\n",
    "            tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "            # Save the loaded model in 'models' to speed up its later use\n",
    "            self.models[model_name] = (model, tokenizer)\n",
    "        else:\n",
    "            model, tokenizer = self.models[model_name]\n",
    "        return model, tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameReplacer:\n",
    "    basic_langs = ['eng', 'spa', 'ita', 'hat', 'por']\n",
    "    \n",
    "    def __init__(self, *, name: str, translator: Translator, data: DataSource, init_lang: str):\n",
    "        assert init_lang in self.basic_langs\n",
    "        self.translator = translator\n",
    "        self.data = data\n",
    "        self.init_lang = init_lang\n",
    "        self.name = name\n",
    "    \n",
    "    \n",
    "    def replace_name(self, *, new_name: str, lang: str, sentence: str, ref_sentence: str = None):\n",
    "        if lang in self.basic_langs:\n",
    "            return self._replace_basic(new_name, sentence, lang)\n",
    "        else:\n",
    "            return self._replace_inflected(new_name, sentence, ref_sentence, lang)\n",
    "    \n",
    "    \n",
    "    def is_basic(self, lang):\n",
    "        return lang in self.basic_langs\n",
    "    \n",
    "\n",
    "    def _replace_basic(self, new_name: str, sentence: str, lang: str):\n",
    "        old_name = self.data.name_pattern(self.name, lang)\n",
    "        return re.sub(old_name, new_name, sentence)\n",
    "\n",
    "\n",
    "    def _replace_inflected(self, new_name: str, sentence: str, ref_sentence: str, lang: str):\n",
    "        assert ref_sentence is not None\n",
    "         \n",
    "        # Translate the original sentence (with the old name) into the target language\n",
    "        ref_translated = self.translator.translate(\n",
    "            ref_sentence, self.data.iso(self.init_lang), self.data.iso(lang)\n",
    "        )\n",
    "        # Replace the name in the original sentence with the new name\n",
    "        ref_replaced = self.replace_name(\n",
    "            new_name=new_name, lang=self.init_lang, sentence=ref_sentence\n",
    "        )\n",
    "        # Translate the previous sentence (with the new name) into the target language \n",
    "        ref_replaced_translated = self.translator.translate(\n",
    "            ref_replaced, self.data.iso(self.init_lang), self.data.iso(lang)\n",
    "        )\n",
    "        \n",
    "        # Get the difference between both translations, hoping that the thing that changed was the name or some\n",
    "        # sorrounding text, in order to find the most likely inflection of the replaced name in the target language\n",
    "        repls = self._find_replacements(ref_translated, ref_replaced_translated)\n",
    "        \n",
    "        #print(ref_translated, ref_replaced_translated, repls)\n",
    "\n",
    "        # Don't replace if it finds more than one replacement, because the ambiguity might cause serious mistakes.\n",
    "        if len(repls) > 1 or len(repls) == 0:\n",
    "            return None\n",
    "\n",
    "        repl = repls[0]\n",
    "        old_name = self.data.name_pattern(self.name, lang)\n",
    "        # If it doesn't detect the name to replace, just return the sentence untouched\n",
    "        if re.search(old_name, repl[0]) is None:\n",
    "            return None\n",
    "\n",
    "        # Replace name by its regular expression to match all variations\n",
    "        name_pattern = re.sub(old_name, old_name, repl[0])\n",
    "        return re.sub(name_pattern, repl[1], sentence)\n",
    "\n",
    "    \n",
    "    def _find_replacements(self, base, replaced): \n",
    "        base = word_tokenize(base)\n",
    "        replaced = word_tokenize(replaced)\n",
    "        s = SequenceMatcher(None, base, replaced)\n",
    "        replacements = []\n",
    "        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "            if tag == 'replace':\n",
    "                replacements.append((' '.join(base[i1:i2]), ' '.join(replaced[j1:j2])))\n",
    "        return replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BulkReplacer:\n",
    "    def __init__(self, *, generator: NameGenerator, data: DataSource):\n",
    "        self.data = data\n",
    "        self.generator = generator\n",
    "        self.init_lang = generator.init_lang\n",
    "        self.translator = Translator()\n",
    "        self.replacer = NameReplacer(\n",
    "            name=generator.name, init_lang=self.init_lang, translator=self.translator, data=data\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def replace_lang_names(self, lang):\n",
    "        lang_sentences = self.data.name_sentences(self.generator.name, lang)\n",
    "        chunks = []\n",
    "        for chunk in tqdm(np.array_split(lang_sentences, len(lang_sentences)//150 + 1)):\n",
    "            refs = []\n",
    "            if not self.replacer.is_basic(lang):\n",
    "                refs = self._prepare_refs(chunk, lang)\n",
    "            chunk['sentence_new'] = chunk.apply(\n",
    "                lambda row: self._replace_row(row, lang, refs), axis=1\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        return pd.concat(chunks)\n",
    "        \n",
    "    \n",
    "    def _prepare_refs(self, chunk, lang):\n",
    "        ids = chunk['tatoeba_id'].to_list()\n",
    "        refs = self.data.find_translations(ids, self.init_lang)\n",
    "        texts = [v for (k,v) in refs.items() if v is not None]\n",
    "        self.translator.cache_translations(texts, self.data.iso(self.init_lang), self.data.iso(lang))\n",
    "        tatoeba_ids = chunk['tatoeba_id'].to_list()\n",
    "        sentences = chunk['sentence'].to_list()\n",
    "        \n",
    "        replaced_refs = []\n",
    "        for tatoeba_id in tatoeba_ids:\n",
    "            if refs[tatoeba_id] is not None:\n",
    "                new_name = self.generator.get_name_replacement(tatoeba_id)\n",
    "                ref_replaced = self.replacer.replace_name(\n",
    "                    new_name=new_name, lang=self.init_lang, sentence=refs[tatoeba_id]\n",
    "                )\n",
    "                replaced_refs.append(ref_replaced)\n",
    "        \n",
    "        self.translator.cache_translations(replaced_refs, self.data.iso(self.init_lang), self.data.iso(lang), fast)\n",
    "        return refs\n",
    "    \n",
    "    \n",
    "    def _replace_row(self, row, lang, refs):\n",
    "        new_name = self.generator.get_name_replacement(row['tatoeba_id'])\n",
    "        ref = refs[row['tatoeba_id']] if row['tatoeba_id'] in refs else None\n",
    "        if ref is None and not self.replacer.is_basic(lang):\n",
    "            ref = self.translator.translate(\n",
    "                row['sentence'], self.data.iso(lang), self.data.iso(self.init_lang)\n",
    "            )\n",
    "        \n",
    "        new_sentence = self.replacer.replace_name(\n",
    "            new_name=new_name, lang=lang, sentence=row['sentence'], ref_sentence=ref\n",
    "        )\n",
    "        \n",
    "        if new_sentence is None:\n",
    "            new_sentence = row['sentence']\n",
    "            self.generator.takeback(row['tatoeba_id'])\n",
    "        return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataSource(data_path='../data/sources/tatoeba', name_patterns=tom_mary, new_names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = NameGenerator(name='Tom', gender='male', init_lang='eng', data=data)\n",
    "#generator.generate()\n",
    "\n",
    "generator.load('../data/objects')\n",
    "\n",
    "#generator.generate()\n",
    "#generator.save('../data/objects')\n",
    "\n",
    "bulk = BulkReplacer(generator=generator, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rus = bulk.replace_lang_names('eng')\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('../data/objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus.to_csv('por_tom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import stanza\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Models that are able to do Named Entity Recognition.\n",
    "ner_languages = ['spa', 'rus', 'deu', 'fra', 'nld']\n",
    "\n",
    "stanza_models = {}\n",
    "\n",
    "def count_people(sentences, lang, regex, sample = 500):\n",
    "    torch.cuda.empty_cache()\n",
    "    nlp = None\n",
    "    if lang in ner_languages:\n",
    "        if lang in stanza_models:\n",
    "            nlp = stanza_models[lang]\n",
    "        else:\n",
    "            nlp = stanza_models[lang] = stanza.Pipeline(iso_map[lang], dir='../data/models/stanza')\n",
    "    \n",
    "    lang_sentences = sentences.loc[sentences['language'] == lang].sample(sample)\n",
    "    \n",
    "    people = collections.Counter()\n",
    "\n",
    "    for index, row in lang_sentences.iterrows():\n",
    "        if nlp is not None:\n",
    "            doc = nlp(row['sentence'])\n",
    "            persons = [entity.text for entity in doc.entities if entity.type == 'PER']\n",
    "        else:\n",
    "            persons = word_tokenize(row['sentence'])\n",
    "        \n",
    "        for person in persons:\n",
    "            if re.match(regex, person):\n",
    "                people[person] += 1\n",
    "\n",
    "    return sorted(people, key=people.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "people = [name for name in count_people(sentences, 'eng', '(Tom|Mar)', 30000)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
